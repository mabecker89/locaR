---
title: "Introduction to locaR"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to locaR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
#library(locaR)
```

# Introduction to the locaR package.

Sound localization is a method for pinpointing the location of a sound source in three-dimensional space. It helps answer the question, where did that sound originate from? Answering this question can be valuable in the fields of ecology and behaviour, as knowing an animal's location can provide important insights into various aspects of their biology. The functions in this package may also work on other sound localization applications (e.g. gunshot localization), but have only been tested for localizing birds. 

This package implements the modified steered response power method of Cobos et al. (2010) to estimate a source location. This is intended to analyze data from multiple time-synchronized recordings made with an array of widely spaced microphones. (Side note: other types of sound localization methods focus on estimating the direction towards an incoming sound using microphones in very close proximity, i.e. a few centimeters. This package does not do direction-of-arrival localization). Besides the actual localization, this package includes various functions to help manage and organize localization data, to help with detecting sounds of interest in a localization array, and to validate localization estimates.

To localize sound sources with this package, there are 3 basic data requirements:

1. Recordings must be synchronized.

The synchronization of recordings is fundamental to accurate location estimates, since the localization algorithm estimates source locations based on the time-difference-of-arrival of sounds at different microphones. It is typically desirable to have microphones synchronized within 1 ms of each other. If microphones are not synchronized, the true time-difference-of-arrival cannot be estimated accurately.

2. Microphone locations must be known (more accurate microphone locations will translate into more accurate source localization).

If microphone locations are not known, the time-difference-of-arrival cannot be accurately translated into source location estimates. Methods for estimating microphone locations include using a tape measure to measure (relative) microphone placement; taking many GPS points and averaging them; and using a survey-grade GPS. The latter option is the best one, as it can provide microphone location estimates with a few cm accuracy.

3. Microphones must be placed within earshot of each other.

Of course, if a sound is only audible at one or two microphones, there will not be sufficient information in the time-difference-of-arrival estimates to estimate the source location. Ideally, a signal should reach at least four microphones. This data requirement will determine the spacing between microphones in the field, but there is no universal rule or best practice. For example, if localizing gunshots or wolf howls, which transmit over long distances, microphones can be spaced much farther apart than if localizing songbird vocalizations, which only transmit short distances. For songbird communities, I have found it best to space microphones by 35-40 meters. But even within songbirds, the songs of different species transmit very different distances.

Achieving the above data collection requirements can be challenging in practice. At the present time, for example, most commercially available recording units are not capable of producing synchronized recordings. Current models (2022) that are capable of doing this are the Wildlife Acoustics SM3 (with GPS attachment), the Wildlife Acoustics SM4TS (with GPS attachment), and the Frontier Labs BAR-LT. In the future, this list will grow, for example there are currently plans to incorporate GPS synchronization into Audiomoth units. Other challenges include acquiring accurate microphone locations, for which it may be desirable to use a centimeter-accurate GPS (e.g. an RTK).

# Sound localization as an art.

I often say that sound localization is an art as much as a science. The reason is that achieving the most accurate source localization estimates requires careful attention to detail and some human involvement. Placing blind trust in the localization algorithm output can sometimes lead to incorrect location estimates. Fortunately, by developing an intuition for localization, erroneous estimates can be identified and either removed or improved. The `locaR` package includes some tools for validating localization outputs, so users can decide how much effort to invest in validating results. Validating results requires human involvement, but can increase data quality; accepting results without validation may increase error, but could dramatically increase data set sizes via increased automation. Regardless of which approach is preferred, it is strongly recommended that sound localization practitioners develop an intuition for localization so they can anticipate when localization is likely to succeed and when or why it may fail.

The `locaR` package is designed to work with any synchronized recordings via the `localize()` function. However, many of the functions, most notably the `localizeSingle()` and `localizeMultiple()` functions, have been written specifically to work with Wildlife Acoustics (SM3 or SM4) recordings; these functions are intended to ease the user's data wrangling and data management burden. 

# The localize() function.

The remainder of this vignette will introduce users to the `localize()` function, which is the backbone of the package. 

## Loading the example data.

Example data included with the `locaR` package was collected from nine synchronized recordings with Wildlife Acoustics SM3 units (with GPS attachment). The basic layout of microphones was in a square 3x3 grid, with adjacent microphones separated by 40 meters. Thus the array covers approximately 0.64 hectares. The environment was a mix of wetlands and forest, and the target species were birds. Note that the example data was converted to .mp3 format to reduce the size of the package; these become Wave objects once read into R.

To access the location of the data, use the `system.file()` function, as follows:

```{r, eval = FALSE}
list.files(system.file('data', package = 'locaR'), pattern = '.mp3', full.names = T)
```

Arranging data for input to the localize function involves three steps: 1) creating a named list of Wave objects; 2) arranging the microphone coordinates in a data frame; and 3) specifying several pieces of ancillary information.

## Creating a named list of Wave objects.

This step can be laborious, because generally speaking the .wav files we work with are long and contain many sound sources. Localizing the entire file would produce nonsensical results. Instead, what we need to do is extract one relevant portion of the .wav file at a time, and feed it into the localize function.

To organize the recordings for input to the localize function, we will need to read them into R to create Wave objects using the tuneR package, then create a named list of Wave objects.


```{r}
#Get a list of file paths to the example data.
filepaths <- list.files(system.file('data', package = 'locaR'), pattern = '.mp3', full.names = T)

#Add location names as names to the vector, to create a named vector of filepaths. This will be useful later.
names(filepaths) <- sapply(strsplit(basename(filepaths), '_'), '[[', 1)
```



#read coordinates.

coordinates <- read.csv(system.file('data', 'Vignette_Coordinates.csv', package = 'solo'), stringsAsFactors = F)

#add stations as row names (useful for later).
row.names(coordinates) <- coordinates$Station

#read detections.

detections <- read.csv(system.file('data', 'Vignette_Detections_20200617_090000.csv', package = 'solo'), stringsAsFactors=F)

#specify locFolder, where jpegs will be created. For this example, I created a folder in my D:/ drive.

locFolder <- "D:/soloVignette"

#now use a loop to localize.

i <- 1
for(i in 1:nrow(detections)) {
  row <- detections[i,]

  if(row$Station1 == "" | is.na(row$Station1)) {next}

  #get names of relevant stations for this detection.
  stationSubset <- unlist(row[1,paste0('Station',1:6)])
  #remove NA stations, if applicable.
  stationSubset <- stationSubset[!is.na(stationSubset)]
  stationSubset <- stationSubset[stationSubset != '']


  #make a new wavList containing only the stations of interest.

  pathSubset = filepaths[stationSubset]


  #use createWavList() to create a list of wave objects.
  #The arguments from and to are from row$From and row$To.
  #Buffer is set to 0.2 seconds (added to either side of the detection)
  #channels can be set to NULL, since we want to use the left channel (default).
  #adjustments can be set to NULL, since all files were well synchronized in advance.
  #We can set index = i, so that if there is an error, we can pinpoint which detection
  #it came from.
  wl <- createWavList(paths = pathSubset, names = stationSubset,
                      from = row$From, to = row$To, buffer = 0.2, index=i)

  #Get low and high frequency.
  F_Low <- row$F_Low
  F_High <- row$F_High

  #make a new coordinates data.frame with only relevant stations.
  #Subsetting by the stations vector ensures that the order is the
  #same as the wl object.

  crd <- coordinates[stationSubset,]

  #Create jpeg name.
  jpg <- paste0(formatC(i,width=4,flag='0'), '.jpeg')

  #localize(). Will leave most parameters at their default values.
  loc <- localize(wavList = wl, coordinates = crd, locFolder = locFolder,
                  F_Low = F_Low, F_High = F_High, jpegName = jpg, keep.SearchMap = T)

  if(i == 1) {OUT = cbind(row,loc$location)} else {OUT = rbind(OUT, cbind(row,loc$location))}

}

write.csv(OUT, file.path(locFolder, 'vignette_localizations.csv'))




